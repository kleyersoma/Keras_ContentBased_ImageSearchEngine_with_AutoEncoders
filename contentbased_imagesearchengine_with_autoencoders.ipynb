{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Keras Content Based Image Search Engine wiht AutoEncoders</h1>\n",
    "<br>\n",
    "<div>\n",
    "    <img src=\"images/0.jpg\">\n",
    "</div>\n",
    "<br>\n",
    "<br>\n",
    "This notebooks provides a guide to how use convolutional autoencoders to develop a Content Based Image Search Engine using Keras and Tensorflow.\n",
    "<br>\n",
    "<br>\n",
    "An Image Search Engine is really similar to text search engines, instead of searching similarities or answers to a text query <i>they are provided images as queries, then they look for similar images.</i>\n",
    "<br>\n",
    "<br>\n",
    "The use of Image Search Engines done with Deep Learning <b>techniques & architectures</b>, can be categorized as a form of <b>unsupervised learning.</b> Some of the reasons why this kind of search engines done with deep learning can be categorized as a form of unsupervised learning, are the next:\n",
    "<ul>\n",
    "    <li>During the training of an AutoEncoder, <i>there is not need of using any class labels.</i></li>\n",
    "    <li>As the AutoEncoder is used to compute the Latent-Space representation for images, this could be interpreted as the \n",
    "        <b>feature vector</b> for an image.</li>\n",
    "    <li>At search time, the distance between the Latent-Space vectors is computed. This distances determines the similarity \n",
    "        between images, the smaller the distance, the more visually relevant the images are.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be divided in the next sections:\n",
    "<ul>\n",
    "    <li>How AutoEncoders can be used for Image Search Engines?</li>\n",
    "    <li>Implementing and Training a Convolutional AutoEncoder.</li>\n",
    "    <li>Building an Image Search Engine, <i>that uses an AutoEncoder.</i></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>How AutoEncoders can be used for Image Search Engines?</h2>\n",
    "<br>\n",
    "<div>\n",
    "    <img src=\"images/1.png\">\n",
    "</div>\n",
    "<br>\n",
    "<b>Missing Section: Related To Steps of the Notebook to Explain How AutoEncoders Can Be Used For Image Search Engine.</b>\n",
    "<br>\n",
    "<br>\n",
    "A briefly explanation of how AutoEncoders work is:\n",
    "<ul>\n",
    "    <li>Accept an input set of data.</li>\n",
    "    <li>Internally an <i>AutoEncoder</i> compresses the input data into a <b>Latent-Space</b> representation, <i>this can be \n",
    "        understood as a sinlge vector that compresses and quantifies the input</i>.</li>\n",
    "    <li>The data from the <b>Latent-Space</b> representation is <b><i>reconstructed</i></b> and it is given as output from the \n",
    "        output layer.</li>\n",
    "</ul>\n",
    "<br>\n",
    "As we are going to build an image search engine with an AutoEncoder, the most important part is the <b>Latent-Space representation vector</b>, because this distances can give us a notion of similarity between images.\n",
    "<br>\n",
    "<br>\n",
    "When the AutoEncoder has been trained to <i>encode images</i>, we are allow to:\n",
    "<ul>\n",
    "    <li>The Encoder Model can be used to compute the <b>Latent-Space</b> representation of each image from the dataset. <i>\n",
    "        <b>This representation serves as the feature vector that quantifies the contents of an image</b></i> and how similar \n",
    "        can be to other images.</li>\n",
    "    <li>By comparing this <b>feature vector</b> from the queried image to all <b>feature vectors</b> in the dataset, this \n",
    "        measurement is usually done with Euclidean or Cosine distance.</li>\n",
    "</ul>\n",
    "<br>\n",
    "Feature vectors that have a <i><b>smaller distance</b></i> will be considered <i><b>more similar</b></i>, while images with a <i><b>larger distance</b></i> will be considered <i><b>less similar</b></i>. After this measurement we can sort the results based on the distance, <i><b>from smallest to largest</b></i> and finally display the image retrievel results to the end user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing Necessary Libraries, Classes and Packages</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Nadamdam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3e450b227491>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNadamdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfashion_mnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuild_montages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Nadamdam'"
     ]
    }
   ],
   "source": [
    "from convautoencoder import ConvAutoEncoder\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam, Nadamdam\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from imutils import build_montages\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Implementing and Training a Convolutional AutoEncoder</h2>\n",
    "<br>\n",
    "The image below is an example of how a standard architecture of an AutoEncoder is: \n",
    "<br>\n",
    "<br>\n",
    "<div>\n",
    "    <img src=\"images/0.png\">\n",
    "</div>\n",
    "<h3>Implementing and AutoEncoder</h3>\n",
    "<br>\n",
    "The file <code>convautoencoder.py</code> is the class <code>ConvAutoEncoder()</code>, this class has a static method called <code>autoencoder_model()</code>, in this class the architecture for the AutoEncoder that we will implement is defined, I'll provide a quick explanation of how the model is:\n",
    "<ul>\n",
    "    <li>The <code>Input</code> is defined for the <code>Encoder</code> as the <code>shape</code> of \n",
    "        <code>width, height, depth</code> then we loop through the <code>filters</code> and add set of \n",
    "        <code>Conv2D -> LeakyReLU -> BatchNormalization</code> layers, all of this is done from line <b>24 to 36</b>.</li>\n",
    "    <li>Then the <code>Encoder</code> is flattened and the <b><i>Latent Vector</i></b> is built, this happens from line \n",
    "        <b>39 to 41</b>. The <b><i>Latent-Space</i></b> representation is the compressed form of data, once the \n",
    "        <code>Encoder</code> is trained, <i>the output of this layer will be the feature vector used to quantify and represent \n",
    "        the contents of the input image.</i></li>\n",
    "    <li>In line <b>44</b> the <code>Decoder</code> starts being built, itwill accept the output of the <code>Encoder</code> \n",
    "        model as its inputs, this output it is the <b><i>Latent Space&Vector</i></b>, looping over <code>filters</code> in \n",
    "        reverse order the <code>Conv2DTranspose -> LeakyReLU -> BatchNormalization</code> layers are built. The construction of \n",
    "        the <code>Decoder</code> starts in line <b>44</b> and it ends in line <b>52</b>.</li>\n",
    "    <li>The orignal <code>depth</code> of the images must be recovered, this is done from lines <b>55 to 56</b>.</li>\n",
    "    <li>Finally the <code>AutoEncoder</code> is constructed in line <b>59</b> and returned from the function in line <b>62</b>.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have understood how the architecture of the model is, now let's create the AutoEncoder model and plot its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = ConvAutoEncoder().autoencoder_model(width=28, height=28, depth=1)\n",
    "plot_model(ae_model, show_shapes=True, show_layer_names=True, dpi=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training the AutoEncoder Model</h3>\n",
    "<br>\n",
    "We have implemented an AutoEncoder, now it is the time to start the training phase, but first let's create some necessary functions, variables and constants for training, predictions and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(decoded, gt, samples=10):\n",
    "    # Initialize the list of output images\n",
    "    outputs = None\n",
    "\n",
    "    # loop over our number of output samples\n",
    "    for i in range(0, samples):\n",
    "        # grab the original image and reconstructed image\n",
    "        original = (gt[i] * 255).astype(\"uint8\")\n",
    "        recon = (decoded[i] * 255).astype(\"uint8\")\n",
    "\n",
    "        # stack the original and reconstructed image side-by-side\n",
    "        output = np.hstack([original, recon])\n",
    "\n",
    "        # if the outputs array is empty, initialize it as the current\n",
    "        # side-by-side image display\n",
    "        if outputs is None:\n",
    "            outputs = output\n",
    "\n",
    "        # otherwise, vertically stack the outputs\n",
    "        else:\n",
    "            outputs = np.vstack([outputs, output])\n",
    "\n",
    "    # return the output images\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Setting Up the Epochs, Initial Learning Rate and Batch Size</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the number of epochs to train for, an initial learning rate and the batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_losses(x, y_tl, y_vl):\n",
    "\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=ae_History.epoch, y=y_tl,\n",
    "                        mode='lines+markers',\n",
    "                        name='Train Loss'))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=ae_History.epoch, y=y_vl,\n",
    "                        mode='lines+markers',\n",
    "                        name='Val Loss'))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': \"Training Loss vs Validation Loss\",\n",
    "            'y':0.9,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'},\n",
    "        xaxis_title=\"Epochs\",\n",
    "        yaxis_title=\"Loss\",\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 16\n",
    "INIT_LR = 1e-3\n",
    "BS = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Importing the Dataset</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that we are going to use for the purposes of this notebook it is the <code>Fashion MNIST</code> dataset, let's imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Fasion MNIST dataset...\")\n",
    "((X_train, _), (X_test, _)) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the shapes of the <code>X's</code> sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: {} and X_Test shape: {}\".format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the intensities of the pixels of the <code>X's</code> sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train pixel intensity:\\n{}\\n\\nX_Test pixel intensity:\\n{}\".format(X_train[0, 0:10, 0:10], X_test[0, 0:10, 0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Some Data Preprocessing</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a channel dimension to every image in the dataset and then scale the pixel intensities to the range [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Adding a Channel Dimension</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the shapes of the X's sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: {} and X_Test shape: {}\".format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Scaling the Pixel Intensities</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_test = X_test.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the intensities of the pixels of the <code>X's</code> sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train pixel intensity:\\n{}\\n\\nX_Test pixel intensity:\\n{}\".format(X_train[0, 0:10, 0:10, 0], X_test[0, 0:10, 0:10, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Adam as Optimizer</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Nadam(lr=INIT_LR, decay=INIT_LR / EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's Compile the AutoEncoder Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.compile(loss=\"mse\", optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Training Time!</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the convolutional autoencoder\n",
    "ae_History = ae_model.fit(X_train,\n",
    "                          X_train,\n",
    "                          validation_data=(X_test, X_test),\n",
    "                          epochs=EPOCHS,\n",
    "                          batch_size=BS,\n",
    "                          use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training loss vs the validation loss, this is done thanks to <code>visualize_losses()</code> method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_losses(x=ae_History.epoch, y_tl=ae_History.history['loss'], y_vl=ae_History.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Predicting the <i>Unknown</i></h4>\n",
    "<br>\n",
    "We are going to use the Convolutional AutoEncoder to make predictions on the testing images, construct the visualization and then save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = ae_model.predict(X_test)\n",
    "visual_predictions = visualize_predictions(decoded, X_test)\n",
    "cv2.imwrite(\"output/visual_predictions.png\", visual_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the method <code>visualizae_predictions()</code> we create an <i>\"image collage\"</i> with the real images and reconstructed images, let's see how the AutoEncoder model is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_preds_img = mpimg.imread(\"output/visual_predictions.png\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(visual_preds_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AutoEncoder is doing a great job of reconstructing the input images, this means that the <b><i>Latent-Space</i></b> representation vectors are doing a good job compressing, quantifying, and representing the input image; <i><b>having such a representation is a requirement when building an image retrieval system.</b></i>\n",
    "<br>\n",
    "<br>\n",
    "If the feature vectors cannot capture and quantify the contents of the image, then there is no way that the Image Search Engine system will be able to return relevant images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Saving the AutoEncoder Model</b>\n",
    "<br>\n",
    "<br>\n",
    "Let's serialize the autoencoder model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.save(\"output/ae_model.h5\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Building an Image Search Engine, that uses an AutoEncoder</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the AutoEncoder model has been successfully trained, it is the time for us to start building the Image Search Engine, therefore we first have to implement an <i><b>Image Indexer using the trained AutoEncoder Model</b></i>.\n",
    "<br>\n",
    "<br>\n",
    "We are going to index the feature extraction that is the output from the <code>Encoder</code> model, therefore we will generate the <b>index of feature vectors</b>, these feature vectors are meant to quantify the contents of each image.\n",
    "<br>\n",
    "<br>\n",
    "It could be that for improving the query speed of the Image Search Engine we would have to implement special data structures such as <a href=\"https://fribbels.github.io/vptree/writeup\">VP-Trees</a> and Random Projection Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start this section by loading the AutoEncoder Model and displaying the summary of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = load_model(\"output/ae_model.h5\")\n",
    "ae_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the encoder model which consists of <i>just</i> the encoder portion of the AutoEncoder, by using the <code>input</code> of the AutoEncoder we are able to create a <code>Model</code> while only accessing the <code>encoder</code> <i>block</i> of the network, in example the <b>Latent-Space feature vector</b> as the <code>output</code> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=ae_model.input, outputs=ae_model.get_layer(\"Encoded\").output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the <b>feature vectors</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = encoder.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only have to construct a dictionary that maps the index of the Fasion_MNIST training image to its corresponding <b>Latent-Space</b> representation; this dictionary consist of:\n",
    "<ul>\n",
    "    <li><code>indexes</code>: Integer indices of each Fashion MNIST image in the dataset.</li>\n",
    "    <li><code>features</code>: The corresponding feature vector for each image in the dataset.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = list(range(0, X_train.shape[0]))\n",
    "data = {\"indexes\": indexes, \"features\": features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to write the data dictionary to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"output/data_dictionary.pickle\", \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Putting All the Pieces Together</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use <code>build_montages</code> script from <code>imutils</code> package to display the AutoEncoder Image Search Engine results. Let's start this part of this notebook by creating some methods&functions.\n",
    "<br>\n",
    "<br>\n",
    "Let's create a function call <code>euclidean</code> that computes and returns the euclidean distance between two vectors, this distance determines the similarity between two feature vectors, in our case it will determine the similarity between feature vectors <code>a</code> and <code>b</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(a, b):\n",
    "    return np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is the <i>Euclidean distance</i>?</b>\n",
    "<br>\n",
    "<br>\n",
    "It is.... and its formula is:\n",
    "<div>\n",
    "</div>\n",
    "<br>\n",
    "Below we define the searching function, this function it is called <code>perform_search</code> and it is responsible for comparing all feature vectors for similarity and returning the results. <code>perform_search</code> takes as parameters <code>queryFeatures</code> which is a feature vector for the query image, <code>index</code> of all features search through and <code>maxResults</code> wich will be the quantity of results to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_search(queryFeatures, index, maxResults=64):\n",
    "    # Initialize our list of results\n",
    "    results = []\n",
    "\n",
    "    # loop over the index\n",
    "    for i in range(0, len(index[\"features\"])):\n",
    "        \"\"\"\n",
    "            Compute the euclidean distance between the query features\n",
    "            and the features for the current image in the index, then\n",
    "            update the results list with a 2-tuple consisting of the\n",
    "            computed distance and the index of the image\n",
    "        \"\"\"\n",
    "        d = euclidean(queryFeatures, index[\"features\"][i])\n",
    "        results.append((d, i))\n",
    "\n",
    "    # Sort the results and grab the top ones\n",
    "    results = sorted(results)[:maxResults]\n",
    "\n",
    "    # Return the list of results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall that the <i><b>Euclidean</b></i> distance determines the similarity of feature vectors, thus it tells us how similar an image is to another, intuitively defines the similarity between images in the next manner:\n",
    "<ul>\n",
    "    <li>The <i>smaller the distance</i> means the <i>more similar</i> the images are.</li>\n",
    "    <li>The <i>bigger the distance</i> means the <i>less similar</i> the images are.</li>\n",
    "</ul>\n",
    "<br>\n",
    "Let's load the indexes from the <code>pickle</code> file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pickle.loads(open(\"output/data_dictionary.pickle\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Processing</b>\n",
    "<br>\n",
    "<br>\n",
    "Usually data processing must be done, at least the query image that we are going to look for similar images, must be processed in the same way we did process the images that we used to train the AutoEncoder model, in our case we have in memory the images that we are going to used, later in this notebook we are going to use real images related to Fashion MNIST, these new images will be processed as we processed the training images.\n",
    "<br>\n",
    "<br>\n",
    "<b>Random Queries</b>\n",
    "<br>\n",
    "<br>\n",
    "Let's randomly sample a set of testing query image indexes, in this case for simplicity we are going to take only 10 random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryIdxs = list(range(0, X_test.shape[0]))\n",
    "queryIdxs = np.random.choice(queryIdxs, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Time Has Come</b>\n",
    "<br>\n",
    "<br>\n",
    "By looping over the images queries in <code>queryIdxs</code>:\n",
    "<ul>\n",
    "    <li>Grab the <code>queryFeatures</code> and perform the search.</li>\n",
    "    <li>Initialize a list to hold the result <code>images</code>.</li>\n",
    "    <li>Loop over the results, scaling the image back to the range <i>[0, 255]</i>, creating an RGB representation from the \n",
    "        grayscale image for display, and then adding it to our.</li>\n",
    "    <li>Display the query image in its own OpenCV window.</li>\n",
    "    <li>Display a <code>montage</code> of search engine results</li>\n",
    "    <li>When the user presses a key, we repeat the process with a different query image; continue to press \n",
    "        a key as you inspect results until all of our query samples have been searched.</-i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the testing indexes\n",
    "for i in queryIdxs:\n",
    "    \"\"\"\n",
    "        Take the features for the current image, find all similar\n",
    "        images in our dataset, and then initialize our list of resultimages\n",
    "    \"\"\"\n",
    "    queryFeatures = features[i]\n",
    "    results = perform_search(queryFeatures, index, maxResults=225)\n",
    "    images = []\n",
    "\n",
    "    # loop over the results\n",
    "    for (d, j) in results:\n",
    "        # grab the result image, convert it back to the range\n",
    "        # [0, 255], and then update the images list\n",
    "        image = (X_train[j] * 255).astype(\"uint8\")\n",
    "        image = np.dstack([image] * 3)\n",
    "        images.append(image)\n",
    "\n",
    "    # Display the query image\n",
    "    query = (X_test[i] * 255).astype(\"uint8\")\n",
    "    cv2.imshow(\"Query\", query)\n",
    "\n",
    "    # Build a montage from the results and display it\n",
    "    montage = build_montages(images, (28, 28), (15, 15))[0]\n",
    "    cv2.imshow(\"Results\", montage)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tips to Improve AutoEncoder Image Search Engine</h2>\n",
    "<br>\n",
    "<ul>\n",
    "    <li>Design a better architecture, <i>the hardest part.</i></li>\n",
    "    <li>Simple Hyperparameter Tuning for <b>learning rate</b> and <b>batch size</b>.</li>\n",
    "    <li>The implementation here is an example of a linear search with <b><i>O(N)</i></b> complexity, meaning that it will not \n",
    "        scale well.\n",
    "        <br>\n",
    "        To improve the speed of the retrieval system, we should use Approximate Nearest Neighbor algorithms and specialized \n",
    "        data structures such as VP-Trees, Random Projection trees, etc., which can reduce the computational complexity to \n",
    "        <b><i>O(log N)</i></b>.</li>\n",
    "</ul>\n",
    "<h2>Summary</h2>\n",
    "<br>\n",
    "This notebook, help us in learning how to use Convolutional AutoEncoders for Image Search Engines using TensorFlow and Keras, to create this engine we used:\n",
    "<ul>\n",
    "    <li>Trained a Convolutional AutoEncoder on the image dataset.</li>\n",
    "    <li>Used the trained AutoEncoder to compute the <b>Latent-Space</b> representation of each image in the dataset, this \n",
    "        representation serves as the feature vector that quantifies the contents of the image.</li>\n",
    "    <li>Compared the feature vector from the query image to all feature vectors in the dataset using a distance function (in \n",
    "        this case, the Euclidean distance, but cosine distance would also work well here). The smaller the distance between the \n",
    "        vectors the more similar our images were.</li>\n",
    "</ul>\n",
    "<br>\n",
    "We then sorted our results based on the computed distance and displayed our results to the user.\n",
    "<br>\n",
    "<br>\n",
    "Autoencoders can be extremely useful for Image Search Engines applications, the downside is that they require a lot of training data, which we may or may not have.\n",
    "<br>\n",
    "<br>\n",
    "More advanced deep learning Image Search Engines systems rely on siamese networks and triplet loss to embed vectors for images such that more similar images lie closer together in a Euclidean space, while less similar images are farther away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .\n",
    "!git commit -m \"Improve Model Architecture\"\n",
    "!git push -u origin master"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
